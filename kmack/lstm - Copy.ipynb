{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_hashes import contractions\n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stopwords = ['stop', 'the', 'to', 'and', 'a', 'in', 'it', 'is', 'I', 'that', 'had', 'on', 'for', 'were', 'was']\n",
    "\n",
    "def preprocess_text(review: str) -> str:\n",
    "    # make everything lowercase\n",
    "    review = review.lower()\n",
    "\n",
    "    # convert contractions to fully written form\n",
    "    review = review.split(' ')\n",
    "    for i in range(len(review)):\n",
    "        try:\n",
    "            exp_form = contractions[f\"{review[i]}\"]\n",
    "\n",
    "            if '/' in exp_form:\n",
    "                exp_form = exp_form.split('/')[random.randint(0, len(exp_form.split('/'))-1)]\n",
    "\n",
    "            review[i] = exp_form\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    review = ' '.join(review).strip()\n",
    "\n",
    "    # remove punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized = tokenizer.tokenize(review)\n",
    "    review = ' '.join(tokenized).strip()\n",
    "    # re.sub(\"[^a-zA-Z]+\", \" \", review).strip()\n",
    "\n",
    "    review = review.replace(\"i m\", \"i am\")\n",
    "\n",
    "    # removing stop words\n",
    "    review = review.split(' ')\n",
    "    denoised_rev = [] \n",
    "    for i in range(len(review)):\n",
    "        if review[i] not in stopwords:\n",
    "            denoised_rev.append(review[i])\n",
    "    \n",
    "    review = ' '.join(denoised_rev)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not much write about here but does exactly wha...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product does exactly as should quite affordabl...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>primary job of this device block breath would ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nice windscreen protects my mxl mic prevents p...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this pop filter great looks performs like stud...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10256</th>\n",
       "      <td>great just as expected thank all</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>i have been thinking about trying nanoweb stri...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>i have tried coated strings past including eli...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10259</th>\n",
       "      <td>well made by elixir developed with taylor guit...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>these strings are really quite good but i woul...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10227 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating\n",
       "0      not much write about here but does exactly wha...     5.0\n",
       "1      product does exactly as should quite affordabl...     5.0\n",
       "2      primary job of this device block breath would ...     5.0\n",
       "3      nice windscreen protects my mxl mic prevents p...     5.0\n",
       "4      this pop filter great looks performs like stud...     5.0\n",
       "...                                                  ...     ...\n",
       "10256                   great just as expected thank all     5.0\n",
       "10257  i have been thinking about trying nanoweb stri...     5.0\n",
       "10258  i have tried coated strings past including eli...     4.0\n",
       "10259  well made by elixir developed with taylor guit...     4.0\n",
       "10260  these strings are really quite good but i woul...     4.0\n",
       "\n",
       "[10227 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_data = pd.read_csv('./data/model_data.csv')\n",
    "model_data = pd.read_csv('./data/Musical_instruments_reviews.csv')\n",
    "model_data = model_data.dropna()\n",
    "\n",
    "# rev = model_data['Translated_Review']\n",
    "rev = list(model_data['reviewText'])\n",
    "\n",
    "# preprocessing text\n",
    "for i in range(len(rev)):\n",
    "    rev[i] = preprocess_text(rev[i])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Review': rev,\n",
    "    'Rating': model_data['overall'] # model_data['Rating']\n",
    "})\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 216.,    0.,  249.,    0.,    0.,  771.,    0., 2079.,    0.,\n",
       "        6912.]),\n",
       " array([1. , 1.4, 1.8, 2.2, 2.6, 3. , 3.4, 3.8, 4.2, 4.6, 5. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT TO SEQUENCES \n",
    "\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "train_iter = iter(AG_NEWS(split='train'))\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8181, 100)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# CREATE SEQUENCE DATASET \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# find most common words\n",
    "words = list(df[\"Review\"])\n",
    "words = pd.DataFrame(' '.join(words).strip().split(' '))\n",
    "\n",
    "top_100 = [w[0] for w in words.value_counts()[:100].index.to_list()]\n",
    "\n",
    "reviews = list(df[\"Review\"])\n",
    "sequences = []\n",
    "for i in range(len(reviews)):\n",
    "    seq = vocab(reviews[i].split(' '))\n",
    "\n",
    "    sequences.append(seq)\n",
    "\n",
    "seqs = []\n",
    "max_seq_len = 100 # max(map(len, sequences))\n",
    "for i in range(len(sequences)):\n",
    "    try:\n",
    "        while len(sequences[i]) < max_seq_len:\n",
    "            sequences[i].insert(0, 0)\n",
    "        while len(sequences[i]) >  max_seq_len:\n",
    "            sequences[i].pop(random.randint(0, len(sequences[i])-1))\n",
    "        sequences[i] = np.array(sequences[i])\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "y = np.array(list(df['Rating']))\n",
    "\n",
    "data = np.array(sequences)\n",
    "\n",
    "ny = []\n",
    "# for i in range(len(y)):\n",
    "#     ny.append((np.array([j+1 for j in range(5)]) == y[i])*1)\n",
    "# y = ny\n",
    "\n",
    "y = np.array(list(df['Rating']))/5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=40)\n",
    "print(X_train.shape)\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "embeddings_api = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "# CREATE EMBEDDING MATRIX\n",
    "\n",
    "embedding_matrix = np.zeros((len(top_100), 100))\n",
    "\n",
    "for i in range(len(embedding_matrix)):\n",
    "    embedding_vector = embeddings_api[top_100[i]]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -88827.47261214,   52932.56317673,   39656.25718291,\n",
       "        -65481.60068638,  -11416.7099419 ,   -1502.18551715,\n",
       "        -73505.11853117,   91431.72896874,  -56708.88102082,\n",
       "        -74419.03056814,   61870.91710804,  -39233.38254305,\n",
       "         18706.50572685,    2548.67124609,  -34969.13375655,\n",
       "        -45925.10569099,   27174.32293374,  -32909.69186741,\n",
       "        -32060.26508506,  120605.64032517,   29019.62513384,\n",
       "          3574.97703315,   61366.10959528,  -21104.37425462,\n",
       "          1330.84545455,  -76821.74550194,  -39479.0472283 ,\n",
       "        -72423.23528569,   21050.1408202 ,  -19277.88622888,\n",
       "        -48197.335047  ,  174663.42258715,   33739.21232575,\n",
       "         56478.9397625 ,  107260.70236691,   83647.99016144,\n",
       "       -113852.99327716,   93164.61961722,   71092.19133074,\n",
       "       -110506.24737308,  -64757.80528593,  -57667.0243663 ,\n",
       "        -64676.68278522,  -24602.02494561,  -44579.44085353,\n",
       "          3223.90066878,  -30071.29693939, -110917.36074533,\n",
       "         -4220.45843928, -194549.59391007,  -60669.39541744,\n",
       "        -58420.79365967,   31121.80890335,  253958.40151506,\n",
       "        -32469.35432171, -533682.38319135,   38199.26114589,\n",
       "        -17939.78631495,  368835.40453017,  103843.65263986,\n",
       "        -18129.41477435,  201716.32853166,  -97791.97284391,\n",
       "        104061.48691913,  155974.57841736,    9385.83818769,\n",
       "        151565.01695796,   15334.67556292,  115702.73625995,\n",
       "        -50545.6774701 ,  -28941.98218983,   -2220.37477272,\n",
       "         37804.10484457, -104311.0528184 ,   13136.83215731,\n",
       "          4284.48830837,  -40425.66192752,  -14210.47810758,\n",
       "       -203194.38322936,  -19097.66835739,  134059.54895651,\n",
       "        -66788.99547535, -137110.38324627,   56475.80127192,\n",
       "       -352363.41940635,   54455.53759161,   30705.53689149,\n",
       "         -8689.74824849,  -54657.37109742,  -41995.14608964,\n",
       "        -21402.42769816,  -40720.82953758, -114784.23319648,\n",
       "          7241.05176959,  -71268.93082578,  -10286.1903693 ,\n",
       "        -83887.449826  , -112382.11983088,  143868.37240725,\n",
       "         89419.10648318])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data and embedding matrix dimensions\n",
    "\n",
    "np.dot(X_train[0], embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.dtype)\n",
    "embedding_matrix = embedding_matrix.astype(np.float64)\n",
    "print(embedding_matrix.dtype)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # self.embed_layer = torch.nn.Parameter(data=torch.Tensor(embedding_matrix), requires_grad=True)\n",
    "        # self.embed_layer = nn.Embedding(100, 100)\n",
    "        # print(embedding_matrix.dtype)\n",
    "        # self.embed_layer.weights = torch.nn.Parameter(torch.tensor(embedding_matrix))\n",
    "        self.l1 = nn.Linear(100, 32)\n",
    "        self.out = nn.Linear(32, 1)# 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.to(device).long())\n",
    "        # x = x.to(device).long()\n",
    "        # print(x)\n",
    "        # x = self.embed_layer(x)\n",
    "        x = x.cpu().detach().numpy()\n",
    "        x = embedding_matrix @ x\n",
    "        # print(x.shape)\n",
    "        x = F.relu(torch.Tensor(x))\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape)\n",
    "model = NN(input_size=X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Data Pt 0 Loss 1.0 ]\n",
      "[ Data Pt 1 Loss 0.0 ]\n",
      "[ Data Pt 2 Loss 0.64000004529953 ]\n",
      "[ Data Pt 3 Loss 1.0 ]\n",
      "[ Data Pt 4 Loss 1.0 ]\n",
      "[ Data Pt 5 Loss 0.0 ]\n",
      "[ Data Pt 6 Loss 0.0 ]\n",
      "[ Data Pt 7 Loss 1.0 ]\n",
      "[ Data Pt 8 Loss 0.64000004529953 ]\n",
      "[ Data Pt 9 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 10 Loss 1.0 ]\n",
      "[ Data Pt 11 Loss 0.64000004529953 ]\n",
      "[ Data Pt 12 Loss 0.1600000113248825 ]\n",
      "[ Data Pt 13 Loss 0.0 ]\n",
      "[ Data Pt 14 Loss 0.0 ]\n",
      "[ Data Pt 15 Loss 0.0 ]\n",
      "[ Data Pt 16 Loss 1.0 ]\n",
      "[ Data Pt 17 Loss 0.0 ]\n",
      "[ Data Pt 18 Loss 0.64000004529953 ]\n",
      "[ Data Pt 19 Loss 1.0 ]\n",
      "[ Data Pt 20 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 21 Loss 1.0 ]\n",
      "[ Data Pt 22 Loss 0.0 ]\n",
      "[ Data Pt 23 Loss 1.0 ]\n",
      "[ Data Pt 24 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 25 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 26 Loss 1.0 ]\n",
      "[ Data Pt 27 Loss 0.0 ]\n",
      "[ Data Pt 28 Loss 0.1600000113248825 ]\n",
      "[ Data Pt 29 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 30 Loss 0.64000004529953 ]\n",
      "[ Data Pt 31 Loss 0.1600000113248825 ]\n",
      "[ Data Pt 32 Loss 0.0 ]\n",
      "[ Data Pt 33 Loss 1.0 ]\n",
      "[ Data Pt 34 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 35 Loss 1.0 ]\n",
      "[ Data Pt 36 Loss 1.0 ]\n",
      "[ Data Pt 37 Loss 0.64000004529953 ]\n",
      "[ Data Pt 38 Loss 1.0 ]\n",
      "[ Data Pt 39 Loss 1.0 ]\n",
      "[ Data Pt 40 Loss 1.0 ]\n",
      "[ Data Pt 41 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 42 Loss 1.0 ]\n",
      "[ Data Pt 43 Loss 1.0 ]\n",
      "[ Data Pt 44 Loss 1.0 ]\n",
      "[ Data Pt 45 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 46 Loss 1.0 ]\n",
      "[ Data Pt 47 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 48 Loss 1.0 ]\n",
      "[ Data Pt 49 Loss 0.64000004529953 ]\n",
      "[ Data Pt 50 Loss 0.0 ]\n",
      "[ Data Pt 51 Loss 1.0 ]\n",
      "[ Data Pt 52 Loss 1.0 ]\n",
      "[ Data Pt 53 Loss 1.0 ]\n",
      "[ Data Pt 54 Loss 1.0 ]\n",
      "[ Data Pt 55 Loss 0.0 ]\n",
      "[ Data Pt 56 Loss 1.0 ]\n",
      "[ Data Pt 57 Loss 1.0 ]\n",
      "[ Data Pt 58 Loss 1.0 ]\n",
      "[ Data Pt 59 Loss 1.0 ]\n",
      "[ Data Pt 60 Loss 1.0 ]\n",
      "[ Data Pt 61 Loss 0.0 ]\n",
      "[ Data Pt 62 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 63 Loss 1.0 ]\n",
      "[ Data Pt 64 Loss 1.0 ]\n",
      "[ Data Pt 65 Loss 1.0 ]\n",
      "[ Data Pt 66 Loss 0.64000004529953 ]\n",
      "[ Data Pt 67 Loss 0.0 ]\n",
      "[ Data Pt 68 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 69 Loss 0.0 ]\n",
      "[ Data Pt 70 Loss 1.0 ]\n",
      "[ Data Pt 71 Loss 1.0 ]\n",
      "[ Data Pt 72 Loss 1.0 ]\n",
      "[ Data Pt 73 Loss 1.0 ]\n",
      "[ Data Pt 74 Loss 0.0 ]\n",
      "[ Data Pt 75 Loss 0.0 ]\n",
      "[ Data Pt 76 Loss 0.64000004529953 ]\n",
      "[ Data Pt 77 Loss 0.0 ]\n",
      "[ Data Pt 78 Loss 0.0 ]\n",
      "[ Data Pt 79 Loss 0.64000004529953 ]\n",
      "[ Data Pt 80 Loss 1.0 ]\n",
      "[ Data Pt 81 Loss 1.0 ]\n",
      "[ Data Pt 82 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 83 Loss 1.0 ]\n",
      "[ Data Pt 84 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 85 Loss 0.64000004529953 ]\n",
      "[ Data Pt 86 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 87 Loss 1.0 ]\n",
      "[ Data Pt 88 Loss 0.64000004529953 ]\n",
      "[ Data Pt 89 Loss 1.0 ]\n",
      "[ Data Pt 90 Loss 0.04000000283122063 ]\n",
      "[ Data Pt 91 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 92 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 93 Loss 1.0 ]\n",
      "[ Data Pt 94 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 95 Loss 0.64000004529953 ]\n",
      "[ Data Pt 96 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 97 Loss 1.0 ]\n",
      "[ Data Pt 98 Loss 0.0 ]\n",
      "[ Data Pt 99 Loss 1.0 ]\n",
      "[ Data Pt 100 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 101 Loss 1.0 ]\n",
      "[ Data Pt 102 Loss 1.0 ]\n",
      "[ Data Pt 103 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 104 Loss 1.0 ]\n",
      "[ Data Pt 105 Loss 1.0 ]\n",
      "[ Data Pt 106 Loss 0.0 ]\n",
      "[ Data Pt 107 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 108 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 109 Loss 1.0 ]\n",
      "[ Data Pt 110 Loss 0.64000004529953 ]\n",
      "[ Data Pt 111 Loss 0.0 ]\n",
      "[ Data Pt 112 Loss 0.0 ]\n",
      "[ Data Pt 113 Loss 0.64000004529953 ]\n",
      "[ Data Pt 114 Loss 1.0 ]\n",
      "[ Data Pt 115 Loss 1.0 ]\n",
      "[ Data Pt 116 Loss 0.0 ]\n",
      "[ Data Pt 117 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 118 Loss 0.64000004529953 ]\n",
      "[ Data Pt 119 Loss 1.0 ]\n",
      "[ Data Pt 120 Loss 0.0 ]\n",
      "[ Data Pt 121 Loss 0.64000004529953 ]\n",
      "[ Data Pt 122 Loss 0.64000004529953 ]\n",
      "[ Data Pt 123 Loss 0.0 ]\n",
      "[ Data Pt 124 Loss 0.64000004529953 ]\n",
      "[ Data Pt 125 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 126 Loss 1.0 ]\n",
      "[ Data Pt 127 Loss 0.04000000283122063 ]\n",
      "[ Data Pt 128 Loss 0.0 ]\n",
      "[ Data Pt 129 Loss 0.05164071172475815 ]\n",
      "[ Data Pt 130 Loss 1.0 ]\n",
      "[ Data Pt 131 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 132 Loss 0.0 ]\n",
      "[ Data Pt 133 Loss 0.64000004529953 ]\n",
      "[ Data Pt 134 Loss 0.64000004529953 ]\n",
      "[ Data Pt 135 Loss 1.0 ]\n",
      "[ Data Pt 136 Loss 1.0 ]\n",
      "[ Data Pt 137 Loss 0.0 ]\n",
      "[ Data Pt 138 Loss 0.0 ]\n",
      "[ Data Pt 139 Loss 1.0 ]\n",
      "[ Data Pt 140 Loss 1.0 ]\n",
      "[ Data Pt 141 Loss 1.0 ]\n",
      "[ Data Pt 142 Loss 0.64000004529953 ]\n",
      "[ Data Pt 143 Loss 0.64000004529953 ]\n",
      "[ Data Pt 144 Loss 0.64000004529953 ]\n",
      "[ Data Pt 145 Loss 0.0 ]\n",
      "[ Data Pt 146 Loss 0.0 ]\n",
      "[ Data Pt 147 Loss 1.0 ]\n",
      "[ Data Pt 148 Loss 0.0 ]\n",
      "[ Data Pt 149 Loss 0.0 ]\n",
      "[ Data Pt 150 Loss 0.0 ]\n",
      "[ Data Pt 151 Loss 1.0 ]\n",
      "[ Data Pt 152 Loss 1.0 ]\n",
      "[ Data Pt 153 Loss 0.0 ]\n",
      "[ Data Pt 154 Loss 0.64000004529953 ]\n",
      "[ Data Pt 155 Loss 1.0 ]\n",
      "[ Data Pt 156 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 157 Loss 1.0 ]\n",
      "[ Data Pt 158 Loss 1.0 ]\n",
      "[ Data Pt 159 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 160 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 161 Loss 0.0 ]\n",
      "[ Data Pt 162 Loss 0.0 ]\n",
      "[ Data Pt 163 Loss 0.5193002820014954 ]\n",
      "[ Data Pt 164 Loss 0.0 ]\n",
      "[ Data Pt 165 Loss 0.0 ]\n",
      "[ Data Pt 166 Loss 0.0 ]\n",
      "[ Data Pt 167 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 168 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 169 Loss 0.0 ]\n",
      "[ Data Pt 170 Loss 0.0 ]\n",
      "[ Data Pt 171 Loss 0.0 ]\n",
      "[ Data Pt 172 Loss 0.0 ]\n",
      "[ Data Pt 173 Loss 0.0 ]\n",
      "[ Data Pt 174 Loss 0.0 ]\n",
      "[ Data Pt 175 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 176 Loss 0.0 ]\n",
      "[ Data Pt 177 Loss 0.0 ]\n",
      "[ Data Pt 178 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 179 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 180 Loss 0.0 ]\n",
      "[ Data Pt 181 Loss 0.0 ]\n",
      "[ Data Pt 182 Loss 0.0 ]\n",
      "[ Data Pt 183 Loss 0.0 ]\n",
      "[ Data Pt 184 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 185 Loss 0.0 ]\n",
      "[ Data Pt 186 Loss 0.0 ]\n",
      "[ Data Pt 187 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 188 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 189 Loss 0.0 ]\n",
      "[ Data Pt 190 Loss 0.0 ]\n",
      "[ Data Pt 191 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 192 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 193 Loss 0.0 ]\n",
      "[ Data Pt 194 Loss 0.0 ]\n",
      "[ Data Pt 195 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 196 Loss 0.0 ]\n",
      "[ Data Pt 197 Loss 0.0 ]\n",
      "[ Data Pt 198 Loss 0.0 ]\n",
      "[ Data Pt 199 Loss 0.0 ]\n",
      "[ Data Pt 200 Loss 0.0 ]\n",
      "[ Data Pt 201 Loss 0.0 ]\n",
      "[ Data Pt 202 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 203 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 204 Loss 0.0 ]\n",
      "[ Data Pt 205 Loss 0.0 ]\n",
      "[ Data Pt 206 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 207 Loss 0.0 ]\n",
      "[ Data Pt 208 Loss 0.0 ]\n",
      "[ Data Pt 209 Loss 0.0 ]\n",
      "[ Data Pt 210 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 211 Loss 0.64000004529953 ]\n",
      "[ Data Pt 212 Loss 0.0 ]\n",
      "[ Data Pt 213 Loss 0.0 ]\n",
      "[ Data Pt 214 Loss 0.0 ]\n",
      "[ Data Pt 215 Loss 0.0 ]\n",
      "[ Data Pt 216 Loss 0.0 ]\n",
      "[ Data Pt 217 Loss 0.0 ]\n",
      "[ Data Pt 218 Loss 0.0 ]\n",
      "[ Data Pt 219 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 220 Loss 0.0 ]\n",
      "[ Data Pt 221 Loss 0.0 ]\n",
      "[ Data Pt 222 Loss 0.0 ]\n",
      "[ Data Pt 223 Loss 0.0 ]\n",
      "[ Data Pt 224 Loss 0.0 ]\n",
      "[ Data Pt 225 Loss 0.0 ]\n",
      "[ Data Pt 226 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 227 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 228 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 229 Loss 0.0 ]\n",
      "[ Data Pt 230 Loss 0.0 ]\n",
      "[ Data Pt 231 Loss 0.0 ]\n",
      "[ Data Pt 232 Loss 0.0 ]\n",
      "[ Data Pt 233 Loss 0.0 ]\n",
      "[ Data Pt 234 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 235 Loss 0.0 ]\n",
      "[ Data Pt 236 Loss 0.0 ]\n",
      "[ Data Pt 237 Loss 0.0 ]\n",
      "[ Data Pt 238 Loss 0.0 ]\n",
      "[ Data Pt 239 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 240 Loss 0.0 ]\n",
      "[ Data Pt 241 Loss 0.0 ]\n",
      "[ Data Pt 242 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 243 Loss 0.0 ]\n",
      "[ Data Pt 244 Loss 0.0 ]\n",
      "[ Data Pt 245 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 246 Loss 0.0 ]\n",
      "[ Data Pt 247 Loss 0.0 ]\n",
      "[ Data Pt 248 Loss 0.0 ]\n",
      "[ Data Pt 249 Loss 0.0 ]\n",
      "[ Data Pt 250 Loss 0.0 ]\n",
      "[ Data Pt 251 Loss 0.0 ]\n",
      "[ Data Pt 252 Loss 0.0 ]\n",
      "[ Data Pt 253 Loss 0.0 ]\n",
      "[ Data Pt 254 Loss 0.0 ]\n",
      "[ Data Pt 255 Loss 0.0 ]\n",
      "[ Data Pt 256 Loss 0.0 ]\n",
      "[ Data Pt 257 Loss 0.0 ]\n",
      "[ Data Pt 258 Loss 0.0 ]\n",
      "[ Data Pt 259 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 260 Loss 0.0 ]\n",
      "[ Data Pt 261 Loss 0.0 ]\n",
      "[ Data Pt 262 Loss 0.0 ]\n",
      "[ Data Pt 263 Loss 0.0 ]\n",
      "[ Data Pt 264 Loss 0.0 ]\n",
      "[ Data Pt 265 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 266 Loss 0.0 ]\n",
      "[ Data Pt 267 Loss 0.0 ]\n",
      "[ Data Pt 268 Loss 0.0 ]\n",
      "[ Data Pt 269 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 270 Loss 0.0 ]\n",
      "[ Data Pt 271 Loss 0.0 ]\n",
      "[ Data Pt 272 Loss 0.0 ]\n",
      "[ Data Pt 273 Loss 0.0 ]\n",
      "[ Data Pt 274 Loss 0.0 ]\n",
      "[ Data Pt 275 Loss 0.0 ]\n",
      "[ Data Pt 276 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 277 Loss 0.0 ]\n",
      "[ Data Pt 278 Loss 0.0 ]\n",
      "[ Data Pt 279 Loss 0.0 ]\n",
      "[ Data Pt 280 Loss 0.0 ]\n",
      "[ Data Pt 281 Loss 0.0 ]\n",
      "[ Data Pt 282 Loss 0.0 ]\n",
      "[ Data Pt 283 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 284 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 285 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 286 Loss 0.0 ]\n",
      "[ Data Pt 287 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 288 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 289 Loss 0.0 ]\n",
      "[ Data Pt 290 Loss 0.0 ]\n",
      "[ Data Pt 291 Loss 0.0 ]\n",
      "[ Data Pt 292 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 293 Loss 0.0 ]\n",
      "[ Data Pt 294 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 295 Loss 0.0 ]\n",
      "[ Data Pt 296 Loss 0.0 ]\n",
      "[ Data Pt 297 Loss 0.0 ]\n",
      "[ Data Pt 298 Loss 0.0 ]\n",
      "[ Data Pt 299 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 300 Loss 0.0 ]\n",
      "[ Data Pt 301 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 302 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 303 Loss 0.0 ]\n",
      "[ Data Pt 304 Loss 0.0 ]\n",
      "[ Data Pt 305 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 306 Loss 0.0 ]\n",
      "[ Data Pt 307 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 308 Loss 0.0 ]\n",
      "[ Data Pt 309 Loss 0.0 ]\n",
      "[ Data Pt 310 Loss 0.0 ]\n",
      "[ Data Pt 311 Loss 0.0 ]\n",
      "[ Data Pt 312 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 313 Loss 0.0 ]\n",
      "[ Data Pt 314 Loss 0.0 ]\n",
      "[ Data Pt 315 Loss 0.0 ]\n",
      "[ Data Pt 316 Loss 0.0 ]\n",
      "[ Data Pt 317 Loss 0.0 ]\n",
      "[ Data Pt 318 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 319 Loss 0.0 ]\n",
      "[ Data Pt 320 Loss 0.0 ]\n",
      "[ Data Pt 321 Loss 0.0 ]\n",
      "[ Data Pt 322 Loss 0.0 ]\n",
      "[ Data Pt 323 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 324 Loss 0.0 ]\n",
      "[ Data Pt 325 Loss 0.0 ]\n",
      "[ Data Pt 326 Loss 0.0 ]\n",
      "[ Data Pt 327 Loss 0.0 ]\n",
      "[ Data Pt 328 Loss 0.08109518140554428 ]\n",
      "[ Data Pt 329 Loss 0.0 ]\n",
      "[ Data Pt 330 Loss 0.0 ]\n",
      "[ Data Pt 331 Loss 0.0 ]\n",
      "[ Data Pt 332 Loss 0.0 ]\n",
      "[ Data Pt 333 Loss 0.0 ]\n",
      "[ Data Pt 334 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 335 Loss 0.0 ]\n",
      "[ Data Pt 336 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 337 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 338 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 339 Loss 0.0 ]\n",
      "[ Data Pt 340 Loss 0.0 ]\n",
      "[ Data Pt 341 Loss 0.0 ]\n",
      "[ Data Pt 342 Loss 0.0 ]\n",
      "[ Data Pt 343 Loss 0.0 ]\n",
      "[ Data Pt 344 Loss 0.0 ]\n",
      "[ Data Pt 345 Loss 0.0 ]\n",
      "[ Data Pt 346 Loss 0.0 ]\n",
      "[ Data Pt 347 Loss 0.0 ]\n",
      "[ Data Pt 348 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 349 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 350 Loss 0.0 ]\n",
      "[ Data Pt 351 Loss 0.0 ]\n",
      "[ Data Pt 352 Loss 0.0 ]\n",
      "[ Data Pt 353 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 354 Loss 0.0 ]\n",
      "[ Data Pt 355 Loss 0.64000004529953 ]\n",
      "[ Data Pt 356 Loss 0.0 ]\n",
      "[ Data Pt 357 Loss 0.0 ]\n",
      "[ Data Pt 358 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 359 Loss 0.0 ]\n",
      "[ Data Pt 360 Loss 0.0 ]\n",
      "[ Data Pt 361 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 362 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 363 Loss 0.0 ]\n",
      "[ Data Pt 364 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 365 Loss 0.0 ]\n",
      "[ Data Pt 366 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 367 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 368 Loss 0.0 ]\n",
      "[ Data Pt 369 Loss 0.0 ]\n",
      "[ Data Pt 370 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 371 Loss 0.0 ]\n",
      "[ Data Pt 372 Loss 0.0 ]\n",
      "[ Data Pt 373 Loss 0.0 ]\n",
      "[ Data Pt 374 Loss 0.0 ]\n",
      "[ Data Pt 375 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 376 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 377 Loss 0.0 ]\n",
      "[ Data Pt 378 Loss 0.0 ]\n",
      "[ Data Pt 379 Loss 0.0 ]\n",
      "[ Data Pt 380 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 381 Loss 0.0 ]\n",
      "[ Data Pt 382 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 383 Loss 0.0 ]\n",
      "[ Data Pt 384 Loss 0.0 ]\n",
      "[ Data Pt 385 Loss 0.0 ]\n",
      "[ Data Pt 386 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 387 Loss 0.0 ]\n",
      "[ Data Pt 388 Loss 0.0 ]\n",
      "[ Data Pt 389 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 390 Loss 0.0 ]\n",
      "[ Data Pt 391 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 392 Loss 0.0 ]\n",
      "[ Data Pt 393 Loss 0.0 ]\n",
      "[ Data Pt 394 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 395 Loss 0.0 ]\n",
      "[ Data Pt 396 Loss 0.0 ]\n",
      "[ Data Pt 397 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 398 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 399 Loss 0.0 ]\n",
      "[ Data Pt 400 Loss 0.0 ]\n",
      "[ Data Pt 401 Loss 0.0 ]\n",
      "[ Data Pt 402 Loss 0.0 ]\n",
      "[ Data Pt 403 Loss 0.0 ]\n",
      "[ Data Pt 404 Loss 0.0 ]\n",
      "[ Data Pt 405 Loss 0.0 ]\n",
      "[ Data Pt 406 Loss 0.0 ]\n",
      "[ Data Pt 407 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 408 Loss 0.0 ]\n",
      "[ Data Pt 409 Loss 0.0 ]\n",
      "[ Data Pt 410 Loss 0.0 ]\n",
      "[ Data Pt 411 Loss 0.0 ]\n",
      "[ Data Pt 412 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 413 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 414 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 415 Loss 0.0 ]\n",
      "[ Data Pt 416 Loss 0.0 ]\n",
      "[ Data Pt 417 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 418 Loss 0.0 ]\n",
      "[ Data Pt 419 Loss 0.0 ]\n",
      "[ Data Pt 420 Loss 0.0 ]\n",
      "[ Data Pt 421 Loss 0.0 ]\n",
      "[ Data Pt 422 Loss 0.0 ]\n",
      "[ Data Pt 423 Loss 0.0 ]\n",
      "[ Data Pt 424 Loss 0.0 ]\n",
      "[ Data Pt 425 Loss 0.0 ]\n",
      "[ Data Pt 426 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 427 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 428 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 429 Loss 0.0 ]\n",
      "[ Data Pt 430 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 431 Loss 0.0 ]\n",
      "[ Data Pt 432 Loss 0.0 ]\n",
      "[ Data Pt 433 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 434 Loss 0.0 ]\n",
      "[ Data Pt 435 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 436 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 437 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 438 Loss 0.0 ]\n",
      "[ Data Pt 439 Loss 0.0 ]\n",
      "[ Data Pt 440 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 441 Loss 0.64000004529953 ]\n",
      "[ Data Pt 442 Loss 0.0 ]\n",
      "[ Data Pt 443 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 444 Loss 0.0 ]\n",
      "[ Data Pt 445 Loss 0.0 ]\n",
      "[ Data Pt 446 Loss 0.0 ]\n",
      "[ Data Pt 447 Loss 0.64000004529953 ]\n",
      "[ Data Pt 448 Loss 0.0 ]\n",
      "[ Data Pt 449 Loss 0.0 ]\n",
      "[ Data Pt 450 Loss 0.0 ]\n",
      "[ Data Pt 451 Loss 0.0 ]\n",
      "[ Data Pt 452 Loss 0.0 ]\n",
      "[ Data Pt 453 Loss 0.0 ]\n",
      "[ Data Pt 454 Loss 0.0 ]\n",
      "[ Data Pt 455 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 456 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 457 Loss 0.0 ]\n",
      "[ Data Pt 458 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 459 Loss 0.0 ]\n",
      "[ Data Pt 460 Loss 0.0 ]\n",
      "[ Data Pt 461 Loss 0.0 ]\n",
      "[ Data Pt 462 Loss 0.0 ]\n",
      "[ Data Pt 463 Loss 0.0 ]\n",
      "[ Data Pt 464 Loss 0.0 ]\n",
      "[ Data Pt 465 Loss 0.0 ]\n",
      "[ Data Pt 466 Loss 0.0 ]\n",
      "[ Data Pt 467 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 468 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 469 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 470 Loss 0.0 ]\n",
      "[ Data Pt 471 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 472 Loss 0.0 ]\n",
      "[ Data Pt 473 Loss 0.0 ]\n",
      "[ Data Pt 474 Loss 0.0 ]\n",
      "[ Data Pt 475 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 476 Loss 0.0 ]\n",
      "[ Data Pt 477 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 478 Loss 0.0 ]\n",
      "[ Data Pt 479 Loss 0.0 ]\n",
      "[ Data Pt 480 Loss 0.0 ]\n",
      "[ Data Pt 481 Loss 0.0 ]\n",
      "[ Data Pt 482 Loss 0.0 ]\n",
      "[ Data Pt 483 Loss 0.0 ]\n",
      "[ Data Pt 484 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 485 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 486 Loss 0.0 ]\n",
      "[ Data Pt 487 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 488 Loss 0.0 ]\n",
      "[ Data Pt 489 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 490 Loss 0.0 ]\n",
      "[ Data Pt 491 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 492 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 493 Loss 0.0 ]\n",
      "[ Data Pt 494 Loss 0.0 ]\n",
      "[ Data Pt 495 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 496 Loss 0.0 ]\n",
      "[ Data Pt 497 Loss 0.0 ]\n",
      "[ Data Pt 498 Loss 0.0 ]\n",
      "[ Data Pt 499 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 500 Loss 0.0 ]\n",
      "[ Data Pt 501 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 502 Loss 0.0 ]\n",
      "[ Data Pt 503 Loss 0.0 ]\n",
      "[ Data Pt 504 Loss 0.0 ]\n",
      "[ Data Pt 505 Loss 0.0 ]\n",
      "[ Data Pt 506 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 507 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 508 Loss 0.0 ]\n",
      "[ Data Pt 509 Loss 0.0 ]\n",
      "[ Data Pt 510 Loss 0.0 ]\n",
      "[ Data Pt 511 Loss 0.0 ]\n",
      "[ Data Pt 512 Loss 0.0 ]\n",
      "[ Data Pt 513 Loss 0.64000004529953 ]\n",
      "[ Data Pt 514 Loss 0.0 ]\n",
      "[ Data Pt 515 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 516 Loss 0.0 ]\n",
      "[ Data Pt 517 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 518 Loss 0.0 ]\n",
      "[ Data Pt 519 Loss 0.0 ]\n",
      "[ Data Pt 520 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 521 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 522 Loss 0.0 ]\n",
      "[ Data Pt 523 Loss 0.0 ]\n",
      "[ Data Pt 524 Loss 0.0 ]\n",
      "[ Data Pt 525 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 526 Loss 0.0 ]\n",
      "[ Data Pt 527 Loss 0.0 ]\n",
      "[ Data Pt 528 Loss 0.0 ]\n",
      "[ Data Pt 529 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 530 Loss 0.0 ]\n",
      "[ Data Pt 531 Loss 0.0 ]\n",
      "[ Data Pt 532 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 533 Loss 0.0 ]\n",
      "[ Data Pt 534 Loss 0.0 ]\n",
      "[ Data Pt 535 Loss 0.0 ]\n",
      "[ Data Pt 536 Loss 0.0 ]\n",
      "[ Data Pt 537 Loss 0.0 ]\n",
      "[ Data Pt 538 Loss 0.0 ]\n",
      "[ Data Pt 539 Loss 0.0 ]\n",
      "[ Data Pt 540 Loss 0.0 ]\n",
      "[ Data Pt 541 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 542 Loss 0.0 ]\n",
      "[ Data Pt 543 Loss 0.0 ]\n",
      "[ Data Pt 544 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 545 Loss 0.0 ]\n",
      "[ Data Pt 546 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 547 Loss 0.0 ]\n",
      "[ Data Pt 548 Loss 0.0 ]\n",
      "[ Data Pt 549 Loss 0.0 ]\n",
      "[ Data Pt 550 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 551 Loss 0.0 ]\n",
      "[ Data Pt 552 Loss 0.0 ]\n",
      "[ Data Pt 553 Loss 0.0 ]\n",
      "[ Data Pt 554 Loss 0.0 ]\n",
      "[ Data Pt 555 Loss 0.0 ]\n",
      "[ Data Pt 556 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 557 Loss 0.0 ]\n",
      "[ Data Pt 558 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 559 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 560 Loss 0.0 ]\n",
      "[ Data Pt 561 Loss 0.0 ]\n",
      "[ Data Pt 562 Loss 0.0 ]\n",
      "[ Data Pt 563 Loss 0.0 ]\n",
      "[ Data Pt 564 Loss 0.0 ]\n",
      "[ Data Pt 565 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 566 Loss 0.0 ]\n",
      "[ Data Pt 567 Loss 0.0 ]\n",
      "[ Data Pt 568 Loss 0.0 ]\n",
      "[ Data Pt 569 Loss 0.0 ]\n",
      "[ Data Pt 570 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 571 Loss 0.0 ]\n",
      "[ Data Pt 572 Loss 0.0 ]\n",
      "[ Data Pt 573 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 574 Loss 0.0 ]\n",
      "[ Data Pt 575 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 576 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 577 Loss 0.0 ]\n",
      "[ Data Pt 578 Loss 0.0 ]\n",
      "[ Data Pt 579 Loss 0.0 ]\n",
      "[ Data Pt 580 Loss 0.0 ]\n",
      "[ Data Pt 581 Loss 0.0 ]\n",
      "[ Data Pt 582 Loss 0.0 ]\n",
      "[ Data Pt 583 Loss 0.0 ]\n",
      "[ Data Pt 584 Loss 0.0 ]\n",
      "[ Data Pt 585 Loss 0.0 ]\n",
      "[ Data Pt 586 Loss 0.0 ]\n",
      "[ Data Pt 587 Loss 0.0 ]\n",
      "[ Data Pt 588 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 589 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 590 Loss 0.0 ]\n",
      "[ Data Pt 591 Loss 0.0 ]\n",
      "[ Data Pt 592 Loss 0.0 ]\n",
      "[ Data Pt 593 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 594 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 595 Loss 0.0 ]\n",
      "[ Data Pt 596 Loss 0.0 ]\n",
      "[ Data Pt 597 Loss 0.0 ]\n",
      "[ Data Pt 598 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 599 Loss 0.0 ]\n",
      "[ Data Pt 600 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 601 Loss 0.0 ]\n",
      "[ Data Pt 602 Loss 0.0 ]\n",
      "[ Data Pt 603 Loss 0.0 ]\n",
      "[ Data Pt 604 Loss 0.0 ]\n",
      "[ Data Pt 605 Loss 0.0 ]\n",
      "[ Data Pt 606 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 607 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 608 Loss 0.0 ]\n",
      "[ Data Pt 609 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 610 Loss 0.0 ]\n",
      "[ Data Pt 611 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 612 Loss 0.0 ]\n",
      "[ Data Pt 613 Loss 0.0 ]\n",
      "[ Data Pt 614 Loss 0.0 ]\n",
      "[ Data Pt 615 Loss 0.0 ]\n",
      "[ Data Pt 616 Loss 0.0 ]\n",
      "[ Data Pt 617 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 618 Loss 0.0 ]\n",
      "[ Data Pt 619 Loss 0.0 ]\n",
      "[ Data Pt 620 Loss 0.0 ]\n",
      "[ Data Pt 621 Loss 0.0 ]\n",
      "[ Data Pt 622 Loss 0.0 ]\n",
      "[ Data Pt 623 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 624 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 625 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 626 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 627 Loss 0.0 ]\n",
      "[ Data Pt 628 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 629 Loss 0.0 ]\n",
      "[ Data Pt 630 Loss 0.0 ]\n",
      "[ Data Pt 631 Loss 0.0 ]\n",
      "[ Data Pt 632 Loss 0.0 ]\n",
      "[ Data Pt 633 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 634 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 635 Loss 0.0 ]\n",
      "[ Data Pt 636 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 637 Loss 0.0 ]\n",
      "[ Data Pt 638 Loss 0.0 ]\n",
      "[ Data Pt 639 Loss 0.0 ]\n",
      "[ Data Pt 640 Loss 0.0 ]\n",
      "[ Data Pt 641 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 642 Loss 0.64000004529953 ]\n",
      "[ Data Pt 643 Loss 0.0 ]\n",
      "[ Data Pt 644 Loss 0.0 ]\n",
      "[ Data Pt 645 Loss 0.0 ]\n",
      "[ Data Pt 646 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 647 Loss 0.0 ]\n",
      "[ Data Pt 648 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 649 Loss 0.0 ]\n",
      "[ Data Pt 650 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 651 Loss 0.0 ]\n",
      "[ Data Pt 652 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 653 Loss 0.0 ]\n",
      "[ Data Pt 654 Loss 0.0 ]\n",
      "[ Data Pt 655 Loss 0.0 ]\n",
      "[ Data Pt 656 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 657 Loss 0.0 ]\n",
      "[ Data Pt 658 Loss 0.0 ]\n",
      "[ Data Pt 659 Loss 0.0 ]\n",
      "[ Data Pt 660 Loss 0.0 ]\n",
      "[ Data Pt 661 Loss 0.0 ]\n",
      "[ Data Pt 662 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 663 Loss 0.0 ]\n",
      "[ Data Pt 664 Loss 0.0 ]\n",
      "[ Data Pt 665 Loss 0.0 ]\n",
      "[ Data Pt 666 Loss 0.0 ]\n",
      "[ Data Pt 667 Loss 0.0 ]\n",
      "[ Data Pt 668 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 669 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 670 Loss 0.0 ]\n",
      "[ Data Pt 671 Loss 0.0 ]\n",
      "[ Data Pt 672 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 673 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 674 Loss 0.0 ]\n",
      "[ Data Pt 675 Loss 0.0 ]\n",
      "[ Data Pt 676 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 677 Loss 0.0 ]\n",
      "[ Data Pt 678 Loss 0.0 ]\n",
      "[ Data Pt 679 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 680 Loss 0.0 ]\n",
      "[ Data Pt 681 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 682 Loss 0.0 ]\n",
      "[ Data Pt 683 Loss 0.0 ]\n",
      "[ Data Pt 684 Loss 0.0 ]\n",
      "[ Data Pt 685 Loss 0.64000004529953 ]\n",
      "[ Data Pt 686 Loss 0.0 ]\n",
      "[ Data Pt 687 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 688 Loss 0.0 ]\n",
      "[ Data Pt 689 Loss 0.0 ]\n",
      "[ Data Pt 690 Loss 0.0 ]\n",
      "[ Data Pt 691 Loss 0.0 ]\n",
      "[ Data Pt 692 Loss 0.0 ]\n",
      "[ Data Pt 693 Loss 0.0 ]\n",
      "[ Data Pt 694 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 695 Loss 0.0 ]\n",
      "[ Data Pt 696 Loss 0.0 ]\n",
      "[ Data Pt 697 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 698 Loss 0.0 ]\n",
      "[ Data Pt 699 Loss 0.0 ]\n",
      "[ Data Pt 700 Loss 0.0 ]\n",
      "[ Data Pt 701 Loss 0.0 ]\n",
      "[ Data Pt 702 Loss 0.0 ]\n",
      "[ Data Pt 703 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 704 Loss 0.0 ]\n",
      "[ Data Pt 705 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 706 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 707 Loss 0.0 ]\n",
      "[ Data Pt 708 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 709 Loss 0.0 ]\n",
      "[ Data Pt 710 Loss 0.0 ]\n",
      "[ Data Pt 711 Loss 0.0 ]\n",
      "[ Data Pt 712 Loss 0.0 ]\n",
      "[ Data Pt 713 Loss 0.0 ]\n",
      "[ Data Pt 714 Loss 0.0 ]\n",
      "[ Data Pt 715 Loss 0.0 ]\n",
      "[ Data Pt 716 Loss 0.0 ]\n",
      "[ Data Pt 717 Loss 0.0 ]\n",
      "[ Data Pt 718 Loss 0.0 ]\n",
      "[ Data Pt 719 Loss 0.0 ]\n",
      "[ Data Pt 720 Loss 0.0 ]\n",
      "[ Data Pt 721 Loss 0.0 ]\n",
      "[ Data Pt 722 Loss 0.0 ]\n",
      "[ Data Pt 723 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 724 Loss 0.0 ]\n",
      "[ Data Pt 725 Loss 0.0 ]\n",
      "[ Data Pt 726 Loss 0.0 ]\n",
      "[ Data Pt 727 Loss 0.0 ]\n",
      "[ Data Pt 728 Loss 0.0 ]\n",
      "[ Data Pt 729 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 730 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 731 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 732 Loss 0.0 ]\n",
      "[ Data Pt 733 Loss 0.0 ]\n",
      "[ Data Pt 734 Loss 0.0 ]\n",
      "[ Data Pt 735 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 736 Loss 0.0 ]\n",
      "[ Data Pt 737 Loss 0.0 ]\n",
      "[ Data Pt 738 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 739 Loss 0.0 ]\n",
      "[ Data Pt 740 Loss 0.0 ]\n",
      "[ Data Pt 741 Loss 0.0 ]\n",
      "[ Data Pt 742 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 743 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 744 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 745 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 746 Loss 0.0 ]\n",
      "[ Data Pt 747 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 748 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 749 Loss 0.0 ]\n",
      "[ Data Pt 750 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 751 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 752 Loss 0.0 ]\n",
      "[ Data Pt 753 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 754 Loss 0.0 ]\n",
      "[ Data Pt 755 Loss 0.0 ]\n",
      "[ Data Pt 756 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 757 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 758 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 759 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 760 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 761 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 762 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 763 Loss 0.0 ]\n",
      "[ Data Pt 764 Loss 0.0 ]\n",
      "[ Data Pt 765 Loss 0.0 ]\n",
      "[ Data Pt 766 Loss 0.0 ]\n",
      "[ Data Pt 767 Loss 0.0 ]\n",
      "[ Data Pt 768 Loss 0.0 ]\n",
      "[ Data Pt 769 Loss 0.0 ]\n",
      "[ Data Pt 770 Loss 0.0 ]\n",
      "[ Data Pt 771 Loss 0.0 ]\n",
      "[ Data Pt 772 Loss 0.0 ]\n",
      "[ Data Pt 773 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 774 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 775 Loss 0.0 ]\n",
      "[ Data Pt 776 Loss 0.0 ]\n",
      "[ Data Pt 777 Loss 0.0 ]\n",
      "[ Data Pt 778 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 779 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 780 Loss 0.0 ]\n",
      "[ Data Pt 781 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 782 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 783 Loss 0.0 ]\n",
      "[ Data Pt 784 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 785 Loss 0.0 ]\n",
      "[ Data Pt 786 Loss 0.64000004529953 ]\n",
      "[ Data Pt 787 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 788 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 789 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 790 Loss 0.0 ]\n",
      "[ Data Pt 791 Loss 0.0 ]\n",
      "[ Data Pt 792 Loss 0.0 ]\n",
      "[ Data Pt 793 Loss 0.0 ]\n",
      "[ Data Pt 794 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 795 Loss 0.0 ]\n",
      "[ Data Pt 796 Loss 0.0 ]\n",
      "[ Data Pt 797 Loss 0.0 ]\n",
      "[ Data Pt 798 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 799 Loss 0.0 ]\n",
      "[ Data Pt 800 Loss 0.0 ]\n",
      "[ Data Pt 801 Loss 0.0 ]\n",
      "[ Data Pt 802 Loss 0.0 ]\n",
      "[ Data Pt 803 Loss 0.0 ]\n",
      "[ Data Pt 804 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 805 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 806 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 807 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 808 Loss 0.0 ]\n",
      "[ Data Pt 809 Loss 0.0 ]\n",
      "[ Data Pt 810 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 811 Loss 0.26546570658683777 ]\n",
      "[ Data Pt 812 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 813 Loss 0.0 ]\n",
      "[ Data Pt 814 Loss 0.0 ]\n",
      "[ Data Pt 815 Loss 0.0 ]\n",
      "[ Data Pt 816 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 817 Loss 0.64000004529953 ]\n",
      "[ Data Pt 818 Loss 0.0 ]\n",
      "[ Data Pt 819 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 820 Loss 0.0 ]\n",
      "[ Data Pt 821 Loss 0.0 ]\n",
      "[ Data Pt 822 Loss 0.0 ]\n",
      "[ Data Pt 823 Loss 0.0 ]\n",
      "[ Data Pt 824 Loss 0.0 ]\n",
      "[ Data Pt 825 Loss 0.0 ]\n",
      "[ Data Pt 826 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 827 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 828 Loss 0.0 ]\n",
      "[ Data Pt 829 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 830 Loss 0.0 ]\n",
      "[ Data Pt 831 Loss 0.0 ]\n",
      "[ Data Pt 832 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 833 Loss 0.64000004529953 ]\n",
      "[ Data Pt 834 Loss 0.0 ]\n",
      "[ Data Pt 835 Loss 0.0 ]\n",
      "[ Data Pt 836 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 837 Loss 0.0 ]\n",
      "[ Data Pt 838 Loss 0.0 ]\n",
      "[ Data Pt 839 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 840 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 841 Loss 0.0 ]\n",
      "[ Data Pt 842 Loss 0.0 ]\n",
      "[ Data Pt 843 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 844 Loss 0.0 ]\n",
      "[ Data Pt 845 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 846 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 847 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 848 Loss 0.0 ]\n",
      "[ Data Pt 849 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 850 Loss 0.0 ]\n",
      "[ Data Pt 851 Loss 0.0 ]\n",
      "[ Data Pt 852 Loss 0.0 ]\n",
      "[ Data Pt 853 Loss 0.0 ]\n",
      "[ Data Pt 854 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 855 Loss 0.0 ]\n",
      "[ Data Pt 856 Loss 0.0 ]\n",
      "[ Data Pt 857 Loss 0.0 ]\n",
      "[ Data Pt 858 Loss 0.0 ]\n",
      "[ Data Pt 859 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 860 Loss 0.0 ]\n",
      "[ Data Pt 861 Loss 0.0 ]\n",
      "[ Data Pt 862 Loss 0.0 ]\n",
      "[ Data Pt 863 Loss 0.0 ]\n",
      "[ Data Pt 864 Loss 0.0 ]\n",
      "[ Data Pt 865 Loss 0.0 ]\n",
      "[ Data Pt 866 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 867 Loss 0.0 ]\n",
      "[ Data Pt 868 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 869 Loss 0.0 ]\n",
      "[ Data Pt 870 Loss 0.0 ]\n",
      "[ Data Pt 871 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 872 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 873 Loss 0.0 ]\n",
      "[ Data Pt 874 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 875 Loss 0.0 ]\n",
      "[ Data Pt 876 Loss 0.0 ]\n",
      "[ Data Pt 877 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 878 Loss 0.0 ]\n",
      "[ Data Pt 879 Loss 0.0 ]\n",
      "[ Data Pt 880 Loss 0.0 ]\n",
      "[ Data Pt 881 Loss 0.0 ]\n",
      "[ Data Pt 882 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 883 Loss 0.0 ]\n",
      "[ Data Pt 884 Loss 0.0 ]\n",
      "[ Data Pt 885 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 886 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 887 Loss 0.0 ]\n",
      "[ Data Pt 888 Loss 0.04000000283122063 ]\n",
      "[ Data Pt 889 Loss 0.0 ]\n",
      "[ Data Pt 890 Loss 0.0 ]\n",
      "[ Data Pt 891 Loss 0.0 ]\n",
      "[ Data Pt 892 Loss 0.0 ]\n",
      "[ Data Pt 893 Loss 0.0 ]\n",
      "[ Data Pt 894 Loss 0.0 ]\n",
      "[ Data Pt 895 Loss 0.0 ]\n",
      "[ Data Pt 896 Loss 0.0 ]\n",
      "[ Data Pt 897 Loss 0.0 ]\n",
      "[ Data Pt 898 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 899 Loss 0.0 ]\n",
      "[ Data Pt 900 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 901 Loss 0.0 ]\n",
      "[ Data Pt 902 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 903 Loss 0.0 ]\n",
      "[ Data Pt 904 Loss 0.0 ]\n",
      "[ Data Pt 905 Loss 0.0 ]\n",
      "[ Data Pt 906 Loss 0.0 ]\n",
      "[ Data Pt 907 Loss 0.0 ]\n",
      "[ Data Pt 908 Loss 0.0 ]\n",
      "[ Data Pt 909 Loss 0.0 ]\n",
      "[ Data Pt 910 Loss 0.0 ]\n",
      "[ Data Pt 911 Loss 0.0 ]\n",
      "[ Data Pt 912 Loss 0.0 ]\n",
      "[ Data Pt 913 Loss 0.0 ]\n",
      "[ Data Pt 914 Loss 0.0 ]\n",
      "[ Data Pt 915 Loss 0.0 ]\n",
      "[ Data Pt 916 Loss 0.0 ]\n",
      "[ Data Pt 917 Loss 0.0 ]\n",
      "[ Data Pt 918 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 919 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 920 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 921 Loss 0.0 ]\n",
      "[ Data Pt 922 Loss 0.0 ]\n",
      "[ Data Pt 923 Loss 0.0 ]\n",
      "[ Data Pt 924 Loss 0.0 ]\n",
      "[ Data Pt 925 Loss 0.0 ]\n",
      "[ Data Pt 926 Loss 0.0 ]\n",
      "[ Data Pt 927 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 928 Loss 0.0 ]\n",
      "[ Data Pt 929 Loss 0.0 ]\n",
      "[ Data Pt 930 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 931 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 932 Loss 0.0 ]\n",
      "[ Data Pt 933 Loss 0.0 ]\n",
      "[ Data Pt 934 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 935 Loss 0.0 ]\n",
      "[ Data Pt 936 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 937 Loss 0.0 ]\n",
      "[ Data Pt 938 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 939 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 940 Loss 0.0 ]\n",
      "[ Data Pt 941 Loss 0.0 ]\n",
      "[ Data Pt 942 Loss 0.0 ]\n",
      "[ Data Pt 943 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 944 Loss 0.0 ]\n",
      "[ Data Pt 945 Loss 0.0 ]\n",
      "[ Data Pt 946 Loss 0.0 ]\n",
      "[ Data Pt 947 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 948 Loss 0.0 ]\n",
      "[ Data Pt 949 Loss 0.0 ]\n",
      "[ Data Pt 950 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 951 Loss 0.0 ]\n",
      "[ Data Pt 952 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 953 Loss 0.0 ]\n",
      "[ Data Pt 954 Loss 0.0 ]\n",
      "[ Data Pt 955 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 956 Loss 0.0 ]\n",
      "[ Data Pt 957 Loss 0.0 ]\n",
      "[ Data Pt 958 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 959 Loss 0.0 ]\n",
      "[ Data Pt 960 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 961 Loss 0.0 ]\n",
      "[ Data Pt 962 Loss 0.0 ]\n",
      "[ Data Pt 963 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 964 Loss 0.0 ]\n",
      "[ Data Pt 965 Loss 0.0 ]\n",
      "[ Data Pt 966 Loss 0.0 ]\n",
      "[ Data Pt 967 Loss 0.0 ]\n",
      "[ Data Pt 968 Loss 0.0 ]\n",
      "[ Data Pt 969 Loss 0.0 ]\n",
      "[ Data Pt 970 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 971 Loss 0.0 ]\n",
      "[ Data Pt 972 Loss 0.0 ]\n",
      "[ Data Pt 973 Loss 0.0 ]\n",
      "[ Data Pt 974 Loss 0.0 ]\n",
      "[ Data Pt 975 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 976 Loss 0.15999998152256012 ]\n",
      "[ Data Pt 977 Loss 0.0 ]\n",
      "[ Data Pt 978 Loss 0.0 ]\n",
      "[ Data Pt 979 Loss 0.0 ]\n",
      "[ Data Pt 980 Loss 0.0 ]\n",
      "[ Data Pt 981 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 982 Loss 0.0 ]\n",
      "[ Data Pt 983 Loss 0.0 ]\n",
      "[ Data Pt 984 Loss 0.0 ]\n",
      "[ Data Pt 985 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 986 Loss 0.0 ]\n",
      "[ Data Pt 987 Loss 0.0 ]\n",
      "[ Data Pt 988 Loss 0.36000001430511475 ]\n",
      "[ Data Pt 989 Loss 0.0 ]\n",
      "[ Data Pt 990 Loss 0.0 ]\n",
      "[ Data Pt 991 Loss 0.0 ]\n",
      "[ Data Pt 992 Loss 0.0 ]\n",
      "[ Data Pt 993 Loss 0.0 ]\n",
      "[ Data Pt 994 Loss 0.0 ]\n",
      "[ Data Pt 995 Loss 0.0 ]\n",
      "[ Data Pt 996 Loss 0.0 ]\n",
      "[ Data Pt 997 Loss 0.03999999538064003 ]\n",
      "[ Data Pt 998 Loss 0.0 ]\n",
      "[ Data Pt 999 Loss 0.0 ]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "\n",
    "criterion = nn.MSELoss() # CrossEntropyLoss() # nn.MSELoss() # RMSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.1)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for t_X, y in zip(X_train, y_train):\n",
    "    t_X = torch.Tensor(t_X)\n",
    "    y = torch.Tensor([y])\n",
    "\n",
    "    preds = model(t_X)\n",
    "\n",
    "    # max_val = float('-inf')\n",
    "    # max_idx = 0\n",
    "    # for i in range(len(preds)):\n",
    "    #     if preds[i] > max_val:\n",
    "    #         max_val = preds[i]\n",
    "    #         max_idx = i\n",
    "\n",
    "    # for i in range(len(y[0])):\n",
    "    #     if y[0][i] == 1:\n",
    "    #         lidx = i\n",
    "    \n",
    "    # label = (y==1).nonzero(as_tuple=True)[0]\n",
    "    # pred_class = (preds==1).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "\n",
    "    # if list(pred_class.size())[0] > 1:\n",
    "    #     pred_class = pred_class[random.randint(0, len(pred_class)-1)]\n",
    "\n",
    "    # pred_class = torch.Tensor([pred_class])\n",
    "\n",
    "    # print(label.cpu().detach().numpy().dtype)\n",
    "    # print(pred_class.cpu().detach().numpy().dtype)\n",
    "\n",
    "    # loss = criterion(torch.Tensor([max_idx]), torch.Tensor([lidx]))\n",
    "    loss = criterion(preds, y)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"[ Data Pt {count} Loss {loss.item()} ]\")\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if count == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19919e39ab0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(len(losses))], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6400], grad_fn=<PowBackward0>)\n",
      "tensor([90.7495], grad_fn=<AddBackward0>)\n",
      "avg loss per pred tensor([0.0444], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nmodel = NN(input_size=100)\n",
    "nmodel.load_state_dict(torch.load('./model'))\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "max_loss = float('-inf')\n",
    "for i in range(len(X_test)):\n",
    "    X = torch.Tensor(X_test[i])\n",
    "    pred = nmodel(X)\n",
    "\n",
    "    curr_loss = ((pred) - (y_test[i])) ** 2 \n",
    "\n",
    "    total_loss += curr_loss\n",
    "\n",
    "    if curr_loss > max_loss:\n",
    "        max_loss = curr_loss\n",
    "\n",
    "print(max_loss)\n",
    "print(total_loss)\n",
    "print(f\"avg loss per pred {total_loss/(len(y_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "crev = \"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\"\n",
    "crev = \"I'm a pro-cheapo and I hated this thing. They're noisy, and the cables feel really cheap, gummy-like. Drop few more bucks and get something else!\"\n",
    "\n",
    "test_ex = vocab(preprocess_text(crev).split(\" \"))\n",
    "while len(test_ex) < 100:\n",
    "    test_ex.insert(0, 0)\n",
    "print(nmodel(torch.Tensor(test_ex))*5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6be3f57eb9c87b6b4f6f83f778a107c8a0106eaf2d3a0c29eb7af43003741456"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
